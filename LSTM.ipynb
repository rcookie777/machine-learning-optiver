{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "HT27bB1StMsQ"
      },
      "outputs": [],
      "source": [
        "#!pip3 install scikeras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r7iXs-3lyE3V"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'keras'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LSTM, Dense, Bidirectional\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam, SGD, RMSprop\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Bidirectional\n",
        "from keras.optimizers import Adam, SGD, RMSprop\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from scikeras.wrappers import KerasClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR7Dcyz-FEbs"
      },
      "source": [
        "##Data Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOnBG6OE_2HU"
      },
      "outputs": [],
      "source": [
        "# Functions I found and adjusted to help with data prep\n",
        "# https://medium.com/@john.kosinski/preparing-and-shaping-timeseries-data-for-keras-lstm-input-part-two-ad17f6ab450\n",
        "\n",
        "# extract X with the given number of timesteps\n",
        "# df: the DataFrame\n",
        "# ntimesteps: number of timesteps\n",
        "#\n",
        "def extract_X(df: pd.DataFrame, ntimesteps: int):\n",
        "    features = len(df.columns)\n",
        "    X = list()\n",
        "\n",
        "    #offset for timesteps\n",
        "    offsets = list()\n",
        "    for i in range (ntimesteps, 0, -1):\n",
        "        offsets.append(df.shift(i))\n",
        "\n",
        "    #combine timestep columns into rows\n",
        "    combined = pd.concat(offsets, axis=1)\n",
        "    combined = combined.tail(-ntimesteps)\n",
        "    combined.drop(combined.tail(1).index, inplace=True)\n",
        "\n",
        "    #reshape each row (timesteps, features)\n",
        "    for i in range(len(combined)):\n",
        "        row = combined.iloc[i].to_numpy()\n",
        "        xrow = list()\n",
        "        for n in range(ntimesteps):\n",
        "            xrow.append(row[n*features:(n*features)+features])\n",
        "        X.append(xrow)\n",
        "\n",
        "    #return as numpy array\n",
        "    return np.array(X)\n",
        "\n",
        "# extract y column (the col to be predicted)\n",
        "# df: the DataFrame\n",
        "# col_name: the name of the column to be predicted\n",
        "# ntimesteps: number of timesteps\n",
        "#\n",
        "def extract_y(df: pd.DataFrame, col_name: str, ntimesteps: int):\n",
        "    shifted = df.shift(-1)\n",
        "    shifted = shifted.head(-2)\n",
        "    shifted = shifted.tail(-(ntimesteps-1))\n",
        "    return shifted[col_name].values\n",
        "\n",
        "class DataSet:\n",
        "    def __init__(self, X, y):\n",
        "        if X.ndim != 3:\n",
        "            raise Exception(\"Expected a 3-dimensional array for X\")\n",
        "        if y.ndim != 1:\n",
        "            raise Exception(\"Expected a 1-dimensional array for y\")\n",
        "        if len(X) != len(y):\n",
        "            raise Exception(\"Length of X and y must be the same\")\n",
        "\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def train_test_split(self, test_size=0.2):\n",
        "        # Calculate the number of samples for testing\n",
        "        num_samples = len(self.X)\n",
        "        num_test_samples = int(num_samples * test_size)\n",
        "\n",
        "        # Shuffle indices\n",
        "        indices = np.random.permutation(num_samples)\n",
        "\n",
        "        # Split indices into training and testing\n",
        "        test_indices = indices[:num_test_samples]\n",
        "        train_indices = indices[num_test_samples:]\n",
        "\n",
        "        # Split X and y based on indices\n",
        "        X_train, y_train = self.X[train_indices], self.y[train_indices]\n",
        "        X_test, y_test = self.X[test_indices], self.y[test_indices]\n",
        "\n",
        "        # Create DataSet objects for training and testing sets\n",
        "        train_data_set = DataSet(X_train, y_train)\n",
        "        test_data_set = DataSet(X_test, y_test)\n",
        "\n",
        "        return train_data_set, test_data_set\n",
        "\n",
        "    @property\n",
        "    def size(self):\n",
        "        return len(self.X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlhOn-2T08PF"
      },
      "outputs": [],
      "source": [
        "# load the dataset\n",
        "df = pd.read_csv(\"/mnt/scratch/tairaeli/cse_dat/train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "qQ9_7THOA_X9",
        "outputId": "73c20ff5-60e7-4aa3-c0f9-751c01157121"
      },
      "outputs": [],
      "source": [
        "# drop na values\n",
        "df.dropna()\n",
        "\n",
        "# calulate ratio\n",
        "df['bid_ask_ratio'] = df['bid_size']/df['ask_size']\n",
        "\n",
        "# create target variable\n",
        "df[\"target_change\"] = np.ones_like(df[\"target\"])\n",
        "df.loc[df[\"target\"]<0,'target_change'] = -1\n",
        "\n",
        "# Select features and target\n",
        "selected_features = ['stock_id', 'ask_size', 'imbalance_size', 'matched_size', 'bid_size', 'bid_ask_ratio', 'reference_price']\n",
        "\n",
        "# use only stock_id 0\n",
        "df = df[df['stock_id'] == 0]\n",
        "df_selected = df[selected_features]\n",
        "target = df[['date_id', 'target_change']]\n",
        "\n",
        "df_selected\n",
        "target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaUe0yhOPbrx",
        "outputId": "de094959-b9c3-4aca-f5b1-5501872cb8e3"
      },
      "outputs": [],
      "source": [
        "# Normalize the data\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(df_selected)\n",
        "# Convert scaled data back to a DataFrame\n",
        "scaled_df = pd.DataFrame(data=scaled_data, columns=df_selected.columns)\n",
        "\n",
        "\n",
        "timesteps = 15\n",
        "# Format data for LSTM input (sample/batches, timesteps, features)\n",
        "dataset = DataSet(extract_X(df_selected, timesteps), extract_y(target, 'target_change', timesteps))\n",
        "dataset.X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8VHG3zRl41m",
        "outputId": "2b1f0a07-5fcd-4caa-85f4-4033cf5dd6f1"
      },
      "outputs": [],
      "source": [
        "# Split into train and test\n",
        "tr_data, te_data = dataset.train_test_split(test_size = .2)\n",
        "tr_data.X.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaLpmut-yE3a"
      },
      "source": [
        "## LSTM Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZGFvbq-rjv3"
      },
      "outputs": [],
      "source": [
        "# Function to create LSTM model\n",
        "def create_lstm_model(neurons=64, activation='relu',\n",
        "                          optimizer='Adam', loss = 'binary_crossentropy',\n",
        "                              learning_rate = 0.001):\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(neurons, activation='relu'), input_shape=(timesteps, tr_data.X.shape[2])))\n",
        "    model.add(Dense(1, activation=activation))\n",
        "\n",
        "    # Set optimizer based on input string\n",
        "    if optimizer == 'Adam':\n",
        "        opt = Adam(learning_rate = learning_rate)\n",
        "    elif optimizer == 'SGD':\n",
        "        opt = SGD(learning_rate = learning_rate)\n",
        "    elif optimizer == 'RMSprop':\n",
        "        opt = RMSprop(learning_rate = learning_rate)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown optimizer: \" + optimizer)\n",
        "\n",
        "    model.compile(optimizer=opt, loss=loss, metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "h8sxXeBuBJX2",
        "outputId": "60f8e9ca-1cba-4c6f-89fd-d2e701530018"
      },
      "outputs": [],
      "source": [
        "# Define parameters for grid search\n",
        "param_grid = {\n",
        "    'neurons': [32, 64, 128],\n",
        "    'activation': ['relu', 'tanh', 'sigmoid', 'linear','swish'],\n",
        "    'optimizer': ['Adam', 'SGD', 'RMSprop'],\n",
        "    'learning_rate': [0.1, 0.01, 0.001, 0.0001]\n",
        "}\n",
        "\n",
        "# Create KerasClassifier wrapper\n",
        "model = KerasClassifier(build_fn=create_lstm_model, epochs=5, verbose=1,\n",
        "                        activation=['relu', 'tanh', 'sigmoid', 'linear','swish'],\n",
        "                        neurons = [32, 64, 128],\n",
        "                        learning_rate = [0.1, 0.01, 0.001, 0.0001])\n",
        "\n",
        "# Perform grid search\n",
        "# change n_jobs to -1 to parallelize or None to go one by one\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs = 6)\n",
        "grid_result = grid.fit(tr_data.X, tr_data.y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdpgQovRFjGH"
      },
      "source": [
        "##Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ucS4uscLtabm"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'grid_result' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Summarize results\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m using \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[43mgrid_result\u001b[49m\u001b[38;5;241m.\u001b[39mbest_score_, grid_result\u001b[38;5;241m.\u001b[39mbest_params_))\n\u001b[1;32m      3\u001b[0m means \u001b[38;5;241m=\u001b[39m grid_result\u001b[38;5;241m.\u001b[39mcv_results_[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_test_score\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m stds \u001b[38;5;241m=\u001b[39m grid_result\u001b[38;5;241m.\u001b[39mcv_results_[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd_test_score\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
            "\u001b[0;31mNameError\u001b[0m: name 'grid_result' is not defined"
          ]
        }
      ],
      "source": [
        "# Summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmtSyK3MHl-C",
        "outputId": "f69e83ab-a75c-4d73-8a64-1d88641c424f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "166/166 [==============================] - 2s 12ms/step - loss: 1.3753 - accuracy: 0.0473\n",
            "Test Loss: 1.3753076791763306\n",
            "Test Accuracy: 0.04728579521179199\n",
            "166/166 [==============================] - 1s 6ms/step\n",
            "[[-1.]\n",
            " [-1.]\n",
            " [-1.]\n",
            " ...\n",
            " [-1.]\n",
            " [-1.]\n",
            " [-1.]]\n"
          ]
        }
      ],
      "source": [
        "model = grid_result.best_estimator\n",
        "loss, accuracy = model.evaluate(te_data.X, te_data.y)\n",
        "\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "# print(model.predict(te_data.X))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
